{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    " # Table of Contents\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\" id=\"toc-level0\"><li><span><a href=\"http://localhost:8888/notebooks/OneDrive/School/PhD/projects/vocalization_classification/pyAudioAnalysis_test/Degu%20Vocalization%20Project.ipynb#Environment\" data-toc-modified-id=\"Environment-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Environment</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/OneDrive/School/PhD/projects/vocalization_classification/pyAudioAnalysis_test/Degu%20Vocalization%20Project.ipynb#Import-all-Libraries,-Packages-needed-to-run-code-below\" data-toc-modified-id=\"Import-all-Libraries,-Packages-needed-to-run-code-below-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Import all Libraries, Packages needed to run code below</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/OneDrive/School/PhD/projects/vocalization_classification/pyAudioAnalysis_test/Degu%20Vocalization%20Project.ipynb#Convert-MATLAB-files-to-python-format\" data-toc-modified-id=\"Convert-MATLAB-files-to-python-format-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Convert MATLAB files to python format</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/OneDrive/School/PhD/projects/vocalization_classification/pyAudioAnalysis_test/Degu%20Vocalization%20Project.ipynb#Save-/-Load-Data-files\" data-toc-modified-id=\"Save-/-Load-Data-files-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Save / Load Data files</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/OneDrive/School/PhD/projects/vocalization_classification/pyAudioAnalysis_test/Degu%20Vocalization%20Project.ipynb#Load-.npy-file\" data-toc-modified-id=\"Load-.npy-file-1.2.1.1\"><span class=\"toc-item-num\">1.2.1.1&nbsp;&nbsp;</span>Load .npy file</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/OneDrive/School/PhD/projects/vocalization_classification/pyAudioAnalysis_test/Degu%20Vocalization%20Project.ipynb#Load-from-.mat-file\" data-toc-modified-id=\"Load-from-.mat-file-1.2.1.2\"><span class=\"toc-item-num\">1.2.1.2&nbsp;&nbsp;</span>Load from .mat file</a></span></li></ul></li></ul></li><li><span><a href=\"http://localhost:8888/notebooks/OneDrive/School/PhD/projects/vocalization_classification/pyAudioAnalysis_test/Degu%20Vocalization%20Project.ipynb#Load-files\" data-toc-modified-id=\"Load-files-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Load files</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/OneDrive/School/PhD/projects/vocalization_classification/pyAudioAnalysis_test/Degu%20Vocalization%20Project.ipynb#Load-Audio-&amp;-Video-Files\" data-toc-modified-id=\"Load-Audio-&amp;-Video-Files-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Load Audio &amp; Video Files</a></span></li></ul></li><li><span><a href=\"http://localhost:8888/notebooks/OneDrive/School/PhD/projects/vocalization_classification/pyAudioAnalysis_test/Degu%20Vocalization%20Project.ipynb#Load-functions\" data-toc-modified-id=\"Load-functions-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Load functions</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/OneDrive/School/PhD/projects/vocalization_classification/pyAudioAnalysis_test/Degu%20Vocalization%20Project.ipynb#file-loading-functions\" data-toc-modified-id=\"file-loading-functions-1.4.1\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>file loading functions</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/OneDrive/School/PhD/projects/vocalization_classification/pyAudioAnalysis_test/Degu%20Vocalization%20Project.ipynb#add-Features-Functions\" data-toc-modified-id=\"add-Features-Functions-1.4.2\"><span class=\"toc-item-num\">1.4.2&nbsp;&nbsp;</span>add Features Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/OneDrive/School/PhD/projects/vocalization_classification/pyAudioAnalysis_test/Degu%20Vocalization%20Project.ipynb#get-start,-end,-duration,-and-merge-short-clips\" data-toc-modified-id=\"get-start,-end,-duration,-and-merge-short-clips-1.4.2.1\"><span class=\"toc-item-num\">1.4.2.1&nbsp;&nbsp;</span>get start, end, duration, and merge short clips</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/OneDrive/School/PhD/projects/vocalization_classification/pyAudioAnalysis_test/Degu%20Vocalization%20Project.ipynb#get-FFT-freq-vs-powerspectrum\" data-toc-modified-id=\"get-FFT-freq-vs-powerspectrum-1.4.2.2\"><span class=\"toc-item-num\">1.4.2.2&nbsp;&nbsp;</span>get FFT freq vs powerspectrum</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/OneDrive/School/PhD/projects/vocalization_classification/pyAudioAnalysis_test/Degu%20Vocalization%20Project.ipynb#get-librosa-mean-features\" data-toc-modified-id=\"get-librosa-mean-features-1.4.2.3\"><span class=\"toc-item-num\">1.4.2.3&nbsp;&nbsp;</span>get librosa mean features</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/OneDrive/School/PhD/projects/vocalization_classification/pyAudioAnalysis_test/Degu%20Vocalization%20Project.ipynb#Get-features-for-each-clip-for-an-entire-audio-file\" data-toc-modified-id=\"Get-features-for-each-clip-for-an-entire-audio-file-1.4.2.4\"><span class=\"toc-item-num\">1.4.2.4&nbsp;&nbsp;</span>Get features for each clip for an entire audio file</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/OneDrive/School/PhD/projects/vocalization_classification/pyAudioAnalysis_test/Degu%20Vocalization%20Project.ipynb#iterate-through-all-audio-files-(must-be-in-single-folder),-saves-data-to-a-CSV\" data-toc-modified-id=\"iterate-through-all-audio-files-(must-be-in-single-folder),-saves-data-to-a-CSV-1.4.2.5\"><span class=\"toc-item-num\">1.4.2.5&nbsp;&nbsp;</span>iterate through all audio files (must be in single folder), saves data to a CSV</a></span></li></ul></li><li><span><a href=\"http://localhost:8888/notebooks/OneDrive/School/PhD/projects/vocalization_classification/pyAudioAnalysis_test/Degu%20Vocalization%20Project.ipynb#audio-playback-functions\" data-toc-modified-id=\"audio-playback-functions-1.4.3\"><span class=\"toc-item-num\">1.4.3&nbsp;&nbsp;</span>audio playback functions</a></span></li></ul></li></ul></li><li><span><a href=\"http://localhost:8888/notebooks/OneDrive/School/PhD/projects/vocalization_classification/pyAudioAnalysis_test/Degu%20Vocalization%20Project.ipynb#Audio-Playback\" data-toc-modified-id=\"Audio-Playback-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Audio Playback</a></span><ul class=\"toc-item\"><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/OneDrive/School/PhD/projects/vocalization_classification/pyAudioAnalysis_test/Degu%20Vocalization%20Project.ipynb#get-aud-file-info\" data-toc-modified-id=\"get-aud-file-info-2.0.1\"><span class=\"toc-item-num\">2.0.1&nbsp;&nbsp;</span>get aud file info</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/OneDrive/School/PhD/projects/vocalization_classification/pyAudioAnalysis_test/Degu%20Vocalization%20Project.ipynb#Playback-by-Index\" data-toc-modified-id=\"Playback-by-Index-2.0.2\"><span class=\"toc-item-num\">2.0.2&nbsp;&nbsp;</span>Playback by Index</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/OneDrive/School/PhD/projects/vocalization_classification/pyAudioAnalysis_test/Degu%20Vocalization%20Project.ipynb#Playback-entire-audio-file\" data-toc-modified-id=\"Playback-entire-audio-file-2.0.3\"><span class=\"toc-item-num\">2.0.3&nbsp;&nbsp;</span>Playback entire audio file</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/OneDrive/School/PhD/projects/vocalization_classification/pyAudioAnalysis_test/Degu%20Vocalization%20Project.ipynb#Playback-by-Call\" data-toc-modified-id=\"Playback-by-Call-2.0.4\"><span class=\"toc-item-num\">2.0.4&nbsp;&nbsp;</span>Playback by Call</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/OneDrive/School/PhD/projects/vocalization_classification/pyAudioAnalysis_test/Degu%20Vocalization%20Project.ipynb#Playback-by-Start-&amp;-End-Times\" data-toc-modified-id=\"Playback-by-Start-&amp;-End-Times-2.0.5\"><span class=\"toc-item-num\">2.0.5&nbsp;&nbsp;</span>Playback by Start &amp; End Times</a></span></li></ul></ul></li><li><span><a href=\"http://localhost:8888/notebooks/OneDrive/School/PhD/projects/vocalization_classification/pyAudioAnalysis_test/Degu%20Vocalization%20Project.ipynb#Extract-Features\" data-toc-modified-id=\"Extract-Features-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Extract Features</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/OneDrive/School/PhD/projects/vocalization_classification/pyAudioAnalysis_test/Degu%20Vocalization%20Project.ipynb#save-features-to-a-CSV-for-training-classifiers\" data-toc-modified-id=\"save-features-to-a-CSV-for-training-classifiers-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>save features to a CSV for training classifiers</a></span></li></ul></li><li><span><a href=\"http://localhost:8888/notebooks/OneDrive/School/PhD/projects/vocalization_classification/pyAudioAnalysis_test/Degu%20Vocalization%20Project.ipynb#Vocalization-Classifier\" data-toc-modified-id=\"Vocalization-Classifier-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Vocalization Classifier</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/OneDrive/School/PhD/projects/vocalization_classification/pyAudioAnalysis_test/Degu%20Vocalization%20Project.ipynb#Import-datasets\" data-toc-modified-id=\"Import-datasets-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Import datasets</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/OneDrive/School/PhD/projects/vocalization_classification/pyAudioAnalysis_test/Degu%20Vocalization%20Project.ipynb#Initialize-all-functions\" data-toc-modified-id=\"Initialize-all-functions-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Initialize all functions</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/OneDrive/School/PhD/projects/vocalization_classification/pyAudioAnalysis_test/Degu%20Vocalization%20Project.ipynb#Initialize-all-classifiers\" data-toc-modified-id=\"Initialize-all-classifiers-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Initialize all classifiers</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/OneDrive/School/PhD/projects/vocalization_classification/pyAudioAnalysis_test/Degu%20Vocalization%20Project.ipynb#Initialize-all-classifier-functions\" data-toc-modified-id=\"Initialize-all-classifier-functions-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Initialize all classifier functions</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/OneDrive/School/PhD/projects/vocalization_classification/pyAudioAnalysis_test/Degu%20Vocalization%20Project.ipynb#create-dictionary-for-grid_search\" data-toc-modified-id=\"create-dictionary-for-grid_search-4.2.3\"><span class=\"toc-item-num\">4.2.3&nbsp;&nbsp;</span>create dictionary for grid_search</a></span></li></ul></li><li><span><a href=\"http://localhost:8888/notebooks/OneDrive/School/PhD/projects/vocalization_classification/pyAudioAnalysis_test/Degu%20Vocalization%20Project.ipynb#Test-Classifiers\" data-toc-modified-id=\"Test-Classifiers-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Test Classifiers</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/OneDrive/School/PhD/projects/vocalization_classification/pyAudioAnalysis_test/Degu%20Vocalization%20Project.ipynb#test-a-single-classifiers-WITH-a-grid_search\" data-toc-modified-id=\"test-a-single-classifiers-WITH-a-grid_search-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>test a single classifiers WITH a grid_search</a></span></li></ul></li><li><span><a href=\"http://localhost:8888/notebooks/OneDrive/School/PhD/projects/vocalization_classification/pyAudioAnalysis_test/Degu%20Vocalization%20Project.ipynb#Detect-Feature-Importance\" data-toc-modified-id=\"Detect-Feature-Importance-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Detect Feature Importance</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/OneDrive/School/PhD/projects/vocalization_classification/pyAudioAnalysis_test/Degu%20Vocalization%20Project.ipynb#Clustering\" data-toc-modified-id=\"Clustering-4.4.1\"><span class=\"toc-item-num\">4.4.1&nbsp;&nbsp;</span>Clustering</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/OneDrive/School/PhD/projects/vocalization_classification/pyAudioAnalysis_test/Degu%20Vocalization%20Project.ipynb#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-4.4.2\"><span class=\"toc-item-num\">4.4.2&nbsp;&nbsp;</span>Confusion Matrix</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T18:32:18.940124Z",
     "start_time": "2017-11-13T18:32:18.938119Z"
    }
   },
   "source": [
    "# Degu Vocalization Project <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T20:10:55.388481Z",
     "start_time": "2017-11-17T20:10:55.380459Z"
    }
   },
   "source": [
    "![alt text](Picture1.gif \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment\n",
    "Python 3.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I you are new to Jupyter Notebook, here's a quick tutorial: \n",
    "\n",
    "https://nbviewer.jupyter.org/github/jupyter/notebook/blob/master/docs/source/examples/Notebook/Running%20Code.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Import all Libraries, Packages needed to run code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T20:18:46.690778Z",
     "start_time": "2017-11-17T20:18:46.668719Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\" run this before all cells (to run a cell press \"ctrl + enter\") \"\"\"\n",
    "\n",
    "import glob, os, itertools, math, wave, struct\n",
    "import librosa, scipy, librosa.display\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.pipeline\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from time import time\n",
    "from IPython.display import Image, display\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "from scipy.io import wavfile as wav\n",
    "from scipy.fftpack import fft\n",
    "\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "\n",
    "from sklearn import model_selection, svm, metrics, preprocessing\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import silhouette_score, fbeta_score, make_scorer, f1_score, accuracy_score, precision_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T18:32:42.932098Z",
     "start_time": "2017-11-13T18:32:42.929090Z"
    },
    "collapsed": true
   },
   "source": [
    "## Convert MATLAB files to python format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T18:32:42.932098Z",
     "start_time": "2017-11-13T18:32:42.929090Z"
    },
    "collapsed": true
   },
   "source": [
    "### Save / Load Data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T20:11:42.431088Z",
     "start_time": "2017-11-20T20:11:42.428080Z"
    }
   },
   "source": [
    "#### Load .npy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load save .npy file\n",
    "filename = \"\"\n",
    "audfilenames = np.load(filename)\n",
    "print (audfilenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Load from .mat file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#load .mat file \n",
    "def load_mat_file(mat_filename, matlab_loadmat_dict):\n",
    "    mat_contents = sio.loadmat(mat_filename)\n",
    "    #remove array from loadmat dict\n",
    "    x = mat_contents[matlab_loadmat_dict]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\" example \"\"\"\n",
    "\n",
    "#load .mat file \n",
    "# mat_filename = 'start_end_for_py.mat'\n",
    "mat_contents_1 = sio.loadmat(r\"/start_end_identnum.mat\")\n",
    "print (mat_contents_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\" example \"\"\"\n",
    "\n",
    "\"\"\" load and print .mat file\n",
    "matlab_loadmat_dict = meta data label in printout \"\"\"\n",
    "\n",
    " matlab_loadmat_dict = \"\"\n",
    "audfilenames = load_mat_file('audfilename.mat', matlab_loadmat_dict)\n",
    "print (audfilenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# remove array from loadmat dict\n",
    "mat_id_file = mat_contents['new_file_']\n",
    "print (mat_id_file.shape)\n",
    "print(mat_id_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# remove array from loadmat dict\n",
    "mat_start_end_id_file = mat_contents_1['matlab_file']\n",
    "print (mat_start_end_id_file.shape)\n",
    "print(mat_start_end_id_file[:,:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# remove array from loadmat dict\n",
    "mat_startend_file = mat_contents['start_end_for_py']\n",
    "print (mat_startend_file.shape)\n",
    "print(mat_startend_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mat_aud_file = mat_contents_0['audfilenames_flip']\n",
    "print (mat_aud_file.shape)\n",
    "print(mat_aud_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T18:46:13.404473Z",
     "start_time": "2017-11-13T18:46:13.402468Z"
    }
   },
   "source": [
    "### Load Audio & Video Files\n",
    "The code cell below will load CSV files with data exported from MATLAB struct\n",
    "    aud_labels includes list of organized audio files with .wav extension\n",
    "    \n",
    "    vid_labels does the same but with .avi video files\n",
    "    \n",
    "    voc_labels includes audio label number that correspond to a label (such as \"warble\")\n",
    "    \n",
    "        each column corresponds to audio files in aud_labels and \n",
    "        \n",
    "        each row correspond to the ordered sequence (i.e. row 4 is the 4th labeled vocaliztion with start and end times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T20:24:24.139238Z",
     "start_time": "2017-11-17T20:24:24.086097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio labels:\n",
      "                                         0\n",
      "0                       1203_1007_24hr.wav\n",
      "1  12030104_100703_shockedaloneREUNION.wav\n",
      "2           12030104_100703_REWARDredo.wav\n",
      "3         12030104_100703_1minute_redo.wav\n",
      "4                  120301_100703_45min.wav\n",
      "0    1203_1007_24hr.wav\n",
      "Name: 0, dtype: object\n",
      "\n",
      "video labels: \n",
      "                                         0\n",
      "0                        1203_1007_24h.avi\n",
      "1  12030104_100703_shockedaloneREUNION.avi\n",
      "2           12030104_100703_REWARDredo.avi\n",
      "3         12030104_100703_1minute_redo.avi\n",
      "4                  120301_100703_45min.avi\n",
      "\n",
      "Vocalizations labels: \n",
      "    0    1     2    3     4    5    6    7    8    9   ...   136  137  138  \\\n",
      "0  30.0  7.0  31.0  9.0  31.0  6.0  6.0  6.0  6.0  6.0 ...   NaN  NaN  NaN   \n",
      "1   1.0  9.0  21.0  6.0  16.0  6.0  6.0  6.0  6.0  6.0 ...   NaN  NaN  NaN   \n",
      "2   6.0  5.0   6.0  6.0  31.0  6.0  6.0  6.0  6.0  6.0 ...   NaN  NaN  NaN   \n",
      "3   6.0  9.0   NaN  6.0  31.0  6.0  6.0  6.0  6.0  6.0 ...   NaN  NaN  NaN   \n",
      "4  25.0  5.0   NaN  6.0  31.0  6.0  6.0  6.0  6.0  6.0 ...   NaN  NaN  NaN   \n",
      "\n",
      "   139  140  141  142  143  144  145  \n",
      "0  6.0  NaN  6.0  1.0  6.0  1.0  6.0  \n",
      "1  6.0  NaN  6.0  6.0  6.0  1.0  6.0  \n",
      "2  6.0  NaN  6.0  6.0  6.0  6.0  6.0  \n",
      "3  1.0  NaN  1.0  1.0  6.0  6.0  1.0  \n",
      "4  1.0  NaN  6.0  1.0  6.0  6.0  6.0  \n",
      "\n",
      "[5 rows x 146 columns]\n"
     ]
    }
   ],
   "source": [
    "# load AUDIO labels as pandas dataframe\n",
    "aud_labels = pd.read_csv(r'/audfile_labels.csv', sep=',',header=None)\n",
    "print ('audio labels:')\n",
    "print (aud_labels.head(n=5))\n",
    "print (aud_labels.loc[0])\n",
    "\n",
    "# load VIDEO labels as pandas dataframe\n",
    "vid_labels = pd.read_csv(r'/vidfile_labels.csv', sep=',',header=None)\n",
    "print ('\\nvideo labels: ')\n",
    "print (vid_labels.head(n=5))\n",
    "\n",
    "# load vocalization labels\n",
    "voc_labels = pd.read_csv(r'/vocalization_labels.csv', sep=',',header=None)\n",
    "print ('\\nVocalizations labels: ')\n",
    "print (voc_labels.head(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load functions \n",
    "\n",
    "These carry out functions call later such as for playing audio files, extracting features etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### file loading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T20:27:13.815777Z",
     "start_time": "2017-11-17T20:27:13.636536Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Returns Call ID and Number from a given .wav file \"\"\"\n",
    "# file start_end times from chitter type\n",
    "def get_chitter_id_num(chitter):\n",
    "    # call ID dictionary\n",
    "    id_dict = {33.0: 'no call', 1.0: 'whistle', 2.0: 'pain', 3.0 : 'bark', \n",
    "               4.0: 'chaff', 5.0: 'chirp', 6.0: 'chitter', \n",
    "               7.0: 'groan', 8.0: 'groan/unknown', 9.0: 'grunt', \n",
    "               10.0: 'high freq chirp', 11.0: 'high freq loud whistle', \n",
    "               12.0: 'high freq squeal', 13.0: 'high freq warble', \n",
    "               14.0: 'long whine', 15.0: 'loud whine', 16.0: 'loud whistle', \n",
    "               17.0: 'low freq', 18.0: 'low freq whistle', 19.0: 'low warble', \n",
    "               20.0: 'low whine', 21.0: 'low whistle', 22.0: 'pain squeak', \n",
    "               23.0: 'pip', 24.0: 'squeak', 25.0: 'squeal', 26.0: 'squeal pain', \n",
    "               27.0: 'tweet', 28.0: 'unknown', 29.0: 'warble', 30.0: 'wheep', \n",
    "               31.0: 'whine', 32.0: 'whine/warble', float('NaN'): 'unlabeled'}\n",
    "    # given Chitter type or call order number \n",
    "\n",
    "    #check for number imput\n",
    "    try:\n",
    "        chitter = float(chitter)\n",
    "        print (str(chitter)+\" = \")\n",
    "        \n",
    "        # checks if in dictionary\n",
    "        if chitter in id_dict:\n",
    "            print (id_dict[chitter])\n",
    "            chitter_id=id_dict[chitter]\n",
    "            chitter_number = chitter\n",
    "            return chitter_id, chitter_number\n",
    "        else:\n",
    "                print (\"\")\n",
    "                print (\"Not a valid number. See below for chitter dictionary:\")\n",
    "                print (id_dict)\n",
    "        # check if in matrix\n",
    "            # if so\n",
    "        # if not \n",
    "            # return error\n",
    "    except ValueError:\n",
    "        print (str(chitter)+\" = \")\n",
    "        if chitter in id_dict.values():\n",
    "            for key, value in id_dict.items():\n",
    "                if chitter == value:\n",
    "                    print (key)\n",
    "                    chitter_number = key\n",
    "                    chitter_id= chitter \n",
    "                    return chitter_id, chitter_number\n",
    "        else:\n",
    "                print (\"\")\n",
    "                print (\"Not a valid ID. See below for valid chitter ID's and #'s:\")\n",
    "                print(\"\")\n",
    "                print (id_dict)\n",
    "        \n",
    "\"\"\" For each call, adds freq features to row \"\"\"        \n",
    "def add_freq_to_df(filepath, filename):\n",
    "    # get rate, data\n",
    "    rate, data = wav.read(filepath)\n",
    "    \n",
    "    # get list of desired signal freq\n",
    "    signal_freq = []\n",
    "    for i in range(1, 41):\n",
    "        x=500*i\n",
    "        signal_freq.append(x)   \n",
    "    print (signal_freq)\n",
    "\n",
    "    # get filename from previous entry\n",
    "    a = get_file_start_end_id(filename)\n",
    "    \n",
    "\n",
    "    song = AudioSegment.from_wav(filepath)\n",
    "    merged_clips = song[0:0]\n",
    "\n",
    "    for index, col in a.iterrows():\n",
    "        print (col['Start Time: '], col['End Time: '])    # add start, end time to a new list\n",
    "\n",
    "        start_clip = int(col['Start Time: ']*1000)\n",
    "        end_clip = int(math.ceil(col['End Time: ']*1000))\n",
    "        print (start_clip, end_clip)\n",
    "        \n",
    "        # read clip in file by start, end\n",
    "        read_clip = song[start_clip:end_clip]\n",
    "        \n",
    "        # array of clip\n",
    "        samples = read_clip.get_array_of_samples()\n",
    "#         info = mediainfo(read_clip)\n",
    "#         print (info['sample_rate'])\n",
    "\n",
    "        \n",
    "        # run welch on sample\n",
    "        y_welch, Pxx_den_w  = scipy.signal.welch(samples, fs=rate, nperseg=512)\n",
    "\n",
    "        \n",
    "        # get dict of features from clip\n",
    "        welch_dict = {str(i):float(np.interp(int(i), y_welch, Pxx_den_w)) for i in signal_freq}\n",
    "\n",
    "        # append to dataframe at index\n",
    "        for key, value in welch_dict.items(): \n",
    "            a.set_value(index, key, value)\n",
    "\n",
    "    # print dataframe\n",
    "    print (a)\n",
    "\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add Features Functions\n",
    "\n",
    "These functions extract features used for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T20:46:34.796592Z",
     "start_time": "2017-11-13T20:38:07.086Z"
    }
   },
   "source": [
    "#### get start, end, duration, and merge short clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T20:46:34.771526Z",
     "start_time": "2017-11-13T20:35:58.989Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "get file start, end times\n",
    "then merges short clips of same type (<500ms apart)\n",
    "and then adds a blank duration labeled \"no-calls\" b/w other vocalizations\n",
    " \n",
    " \"\"\"\n",
    "\n",
    "def get_file_start_end_id_add(filename):\n",
    "    \"\"\" merge same calls of short duration that are adjacen \"\"\"\n",
    "    id_dict = {33.0: 'no call', 1.0: 'whistle', 2.0: 'pain', 3.0 : 'bark', \n",
    "               4.0: 'chaff', 5.0: 'chirp', 6.0: 'chitter', \n",
    "               7.0: 'groan', 8.0: 'groan/unknown', 9.0: 'grunt', \n",
    "               10.0: 'high freq chirp', 11.0: 'high freq loud whistle', \n",
    "               12.0: 'high freq squeal', 13.0: 'high freq warble', \n",
    "               14.0: 'long whine', 15.0: 'loud whine', 16.0: 'loud whistle', \n",
    "               17.0: 'low freq', 18.0: 'low freq whistle', 19.0: 'low warble', \n",
    "               20.0: 'low whine', 21.0: 'low whistle', 22.0: 'pain squeak', \n",
    "               23.0: 'pip', 24.0: 'squeak', 25.0: 'squeal', 26.0: 'squeal pain', \n",
    "               27.0: 'tweet', 28.0: 'unknown', 29.0: 'warble', 30.0: 'wheep', \n",
    "               31.0: 'whine', 32.0: 'whine/warble', float('NaN'): 'unlabeled'}\n",
    "\n",
    "    # load aud_file labels\n",
    "    aud_labels = pd.read_csv(r'\\audfile_labels.csv', sep=',',header=None)\n",
    "\n",
    "    # given filename, get file number to index\n",
    "    filenumber = []\n",
    "    filenumber = aud_labels.index[aud_labels.loc[:,0] == filename].tolist()\n",
    "\n",
    "    print ('\\n'+ 'Experiment Number:  '+str(filenumber[0]) +'\\n')\n",
    "    print ('Media File:         '+str(filename) +\"\\n\")\n",
    "\n",
    "    # load .np exeperiments start / end time array\n",
    "    audfile_start_end_id_array = np.load(r\"/aud_start_end_id_file.npy\")\n",
    "\n",
    "    # index experiment from extracted file number\n",
    "    audfile_start_end_id_df = pd.DataFrame(audfile_start_end_id_array[:, :, int(filenumber[0])])\n",
    "\n",
    "    # remove all NaN's from columns 0 & 1 only\n",
    "    audfile_start_end_id_df.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "    # add call name to number via dict\n",
    "    audfile_start_end_id_df[3] = audfile_start_end_id_df[2].map(id_dict)\n",
    "\n",
    "    # rename header of df\n",
    "    audfile_start_end_id_df.columns =  [\"Start Time: \", \"End Time: \", \"Call ID #: \", \"Call Name: \"]\n",
    "\n",
    "    \"\"\" add 'noncalls' b/w all calls, add start end times \"\"\"\n",
    "#     print ('check', audfile_start_end_id_df['End Time: '], audfile_start_end_id_df['Start Time: '])\n",
    "    audfile_start_end_id_df['Duration_ms'] = (audfile_start_end_id_df['End Time: '].sub(audfile_start_end_id_df['Start Time: '], axis=0)*1000)\n",
    "    audfile_start_end_id_df['Duration_ms'] = audfile_start_end_id_df['Duration_ms'].astype(int)\n",
    "\n",
    "    drop=0\n",
    "    for row, index in audfile_start_end_id_df.iterrows():    \n",
    "        #iterate through, merge if <500ms, same call, & adjacent (<1000ms apart)\n",
    "        row_drop = (row-drop)\n",
    "        if row_drop+1 < len(audfile_start_end_id_df.index):\n",
    "            try:\n",
    "                if int(audfile_start_end_id_df.loc[row_drop, \"Duration_ms\"])<500 and \\\n",
    "                audfile_start_end_id_df.loc[row_drop, \"Call Name: \"] == \\\n",
    "                audfile_start_end_id_df.loc[row_drop+1, \"Call Name: \"] and \\\n",
    "                (int(audfile_start_end_id_df.loc[row_drop+1, \"Start Time: \"]) - \\\n",
    "                 int(audfile_start_end_id_df.loc[row_drop, \"End Time: \"]))<1000:\n",
    "                    # merge rows\n",
    "                    # convert initial row end time to next call's \n",
    "                    audfile_start_end_id_df.set_value(row_drop, \"End Time: \", \n",
    "                                                      audfile_start_end_id_df.get_value(row_drop+1, \"End Time: \"))\n",
    "\n",
    "                    # drop next row\n",
    "                    audfile_start_end_id_df.drop(audfile_start_end_id_df.index[row_drop+1], inplace=True)\n",
    "                    drop +=1\n",
    "                    # reindex\n",
    "                    audfile_start_end_id_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "                else:\n",
    "                    # reindex\n",
    "                    audfile_start_end_id_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            except:\n",
    "                break\n",
    "\n",
    "    # reindex\n",
    "    audfile_start_end_id_df.reset_index(drop=True, inplace=True)\n",
    "    # print (audfile_start_end_id_df)\n",
    "    \n",
    "    count = 0\n",
    "    for row, index in audfile_start_end_id_df.iterrows():        \n",
    "\n",
    "        count +=1\n",
    "        row_count = int(int(row) )\n",
    "\n",
    "        new_row = pd.DataFrame(columns=[\"Start Time: \", \"End Time: \", \"Call ID #: \", \"Call Name: \"])\n",
    "\n",
    "        if 2*row+1 < len(audfile_start_end_id_df.index):\n",
    "            new_row.loc[0] =[audfile_start_end_id_df.loc[row_count, 'End Time: '], \n",
    "                     audfile_start_end_id_df.loc[row_count+1, 'Start Time: '], \n",
    "                     0.0, 'no_call']\n",
    "            audfile_start_end_id_df = pd.concat([audfile_start_end_id_df.loc[:row_count], new_row, audfile_start_end_id_df.loc[row_count+1:]])\n",
    "    #         count +=1\n",
    "    audfile_start_end_id_df['Duration_ms'] = (audfile_start_end_id_df['End Time: '].sub(audfile_start_end_id_df['Start Time: '], axis=0)*1000)\n",
    "    audfile_start_end_id_df['Duration_ms'] = audfile_start_end_id_df['Duration_ms'].astype(int)\n",
    "\n",
    "    audfile_start_end_id_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # print (audfile_start_end_id_df)\n",
    "    return audfile_start_end_id_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get FFT freq vs powerspectrum\n",
    "adds welch filtered freq powerspectrum & duration for each call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T20:46:34.795589Z",
     "start_time": "2017-11-13T20:36:52.653Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" adds welch filtered freq powerspectrum & duration for each call \"\"\"   \n",
    "\n",
    "def add_freq_dur_to_df(filename):\n",
    "    # get rate, data\n",
    "    rate, data = wav.read(filename)\n",
    "    \n",
    "    # get list of desired signal freq\n",
    "    signal_freq = []\n",
    "    for i in range(250, 20250, 250):\n",
    "        signal_freq.append(i)   \n",
    "#     print (signal_freq)\n",
    "\n",
    "    # get filename from previous entry\n",
    "    a = get_file_start_end_id_add(filename)\n",
    "\n",
    "    song = AudioSegment.from_wav(filename)\n",
    "    merged_clips = song[0:0]\n",
    "\n",
    "    \"\"\" get welch filterd FFT powerspectrum freq. \"\"\"\n",
    "\n",
    "    for index, col in a.iterrows():\n",
    "        # print (col['Start Time: '], col['End Time: '])    # add start, end time to a new list\n",
    "\n",
    "        start_clip = int(col['Start Time: ']*1000)\n",
    "        end_clip = int(math.ceil(col['End Time: ']*1000))\n",
    "        # print (start_clip, end_clip)\n",
    "        \n",
    "        # read clip in file by start, end\n",
    "        read_clip = song[start_clip:end_clip]\n",
    "        \n",
    "        # array of clip\n",
    "        samples = read_clip.get_array_of_samples()\n",
    "        \n",
    "        # print (\"read_clip:\")\n",
    "        # print (read_clip)\n",
    "                \n",
    "        # run welch on sample\n",
    "        y_welch, Pxx_den_w  = scipy.signal.welch(samples, fs=rate, nperseg=512)\n",
    " \n",
    "        # get dict of features from clip\n",
    "        try:\n",
    "            welch_dict = {str(i):float(np.interp(int(i), y_welch, Pxx_den_w)) for i in signal_freq}\n",
    "        except:\n",
    "            welch_dict = {str(i):float(0) for i in signal_freq}\n",
    "            \n",
    "        # append to dataframe at index\n",
    "        for key, value in welch_dict.items(): \n",
    "            a.set_value(index, key, value)\n",
    "\n",
    "    \"\"\" get time domain features \"\"\"\n",
    "\n",
    "    for index, col in a.iterrows():\n",
    "\n",
    "        start_clip = int(col['Start Time: ']*1000)\n",
    "        end_clip = int(math.ceil(col['End Time: ']*1000))\n",
    "        # print (start_clip, end_clip)\n",
    "        \n",
    "        # read clip in file by start, end\n",
    "        read_clip = song[start_clip:end_clip]\n",
    "        \n",
    "        # array of clip\n",
    "        samples = read_clip.get_array_of_samples()\n",
    "        \n",
    "        # run welch on sample\n",
    "        y_welch, Pxx_den_w  = scipy.signal.welch(samples, fs=rate, nperseg=512)\n",
    " \n",
    "        # get dict of features from clip\n",
    "        try:\n",
    "            welch_dict = {str(i):float(np.interp(int(i), y_welch, Pxx_den_w)) for i in signal_freq}\n",
    "        except:\n",
    "            welch_dict = {str(i):float(0) for i in signal_freq}\n",
    "            \n",
    "        # append to dataframe at index\n",
    "        for key, value in welch_dict.items(): \n",
    "            a.set_value(index, key, value)\n",
    "            \n",
    "    \"\"\" get standardized freq values from welch freq. \"\"\"\n",
    "    # Create a minimum and maximum processor object\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "    # Create an object to transform the data to fit minmax processor\n",
    "    for column in signal_freq:\n",
    "\n",
    "        x = a[str(column)].values\n",
    "        x = np.reshape(x, (len(x),1))\n",
    "\n",
    "        # standardize column\n",
    "        x_scaled = min_max_scaler.fit_transform(x)\n",
    "\n",
    "        # convert to dataframe type\n",
    "        x_scaled = pd.DataFrame(x_scaled, columns=['0'])\n",
    "\n",
    "        # add new standardaized colomn with S_name\n",
    "        a[\"s\"+str(column)] = x_scaled['0']\n",
    "    \n",
    "    print (\"\\nfinished loading standardized freq\")\n",
    "    \n",
    "    \"\"\" add additional feature functions here \"\"\"\n",
    "    \"\"\" get dominent freq values \"\"\"\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get librosa mean features\n",
    "these get means of different spectral features using librosa over the length of a clip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" these get means of different spectral features using librosa over the length of a clip \"\"\"\n",
    "def extract_feature(file_name, start, duration):\n",
    "    \n",
    "    try:\n",
    "        X, sample_rate = librosa.load(file_name, sr=None, offset=start, duration=duration)\n",
    "    \n",
    "        stft = np.abs(librosa.stft(X))\n",
    "        \n",
    "#         hop_length = duration*sample_rate/100\n",
    "        \n",
    "#         oenv = librosa.onset.onset_strength(y=X, sr=sample_rate, hop_length=hop_length)\n",
    "#         tempogram = librosa.feature.tempogram(onset_envelope=oenv, sr=sample_rate, hop_length=hop_length)\n",
    "        \n",
    "#         tempo = librosa.beat.tempo(onset_envelope=oenv, sr=sample_rate, hop_length=hop_length)[0]\n",
    "\n",
    "#         ac_global = librosa.autocorrelate(oenv, max_size=tempogram.shape[0])\n",
    "#         ac_global = librosa.util.normalize(ac_global)\n",
    "         \n",
    "    #     mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=30).T,axis=0)\n",
    "        mfccs = sp.median(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=30).T,axis=0)\n",
    "\n",
    "    #     chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "        chroma = sp.median(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "\n",
    "    #     mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate, n_mels=64).T,axis=0)\n",
    "        mel = sp.median(librosa.feature.melspectrogram(X, sr=sample_rate, n_mels=64).T,axis=0)\n",
    "\n",
    "    #     contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T,axis=0)\n",
    "        contrast = sp.median(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T,axis=0)\n",
    "\n",
    "    #     tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X), sr=sample_rate).T,axis=0)\n",
    "        tonnetz = sp.median(librosa.feature.tonnetz(y=librosa.effects.harmonic(X), sr=sample_rate).T,axis=0)\n",
    "        \n",
    "#         ac_global_med = sp.median(ac_global)\n",
    "#         print (\"ac_global shape: \", ac_global.shape)\n",
    "        return mfccs,chroma, mel,contrast,tonnetz\n",
    "    \n",
    "    except: \n",
    "        mfccs = np.empty((30,))\n",
    "        mfccs[:] = np.NAN\n",
    "        \n",
    "        chroma = np.empty((12,))\n",
    "        chroma[:] = np.NAN\n",
    "        \n",
    "        mel = np.empty((64,))\n",
    "        mel[:] = np.NAN\n",
    "\n",
    "        contrast = np.empty((7,))\n",
    "        contrast[:] = np.NAN\n",
    "        \n",
    "        tonnetz = np.empty((6,))\n",
    "        tonnetz[:] = np.NAN\n",
    "        \n",
    "#         ac_global = np.empty((100,))\n",
    "#         ac_global[:] = np.NAN\n",
    "        \n",
    "        return mfccs,chroma,mel,contrast,tonnetz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T20:35:03.158816Z",
     "start_time": "2017-11-17T20:35:03.148790Z"
    }
   },
   "source": [
    "#### Get features for each clip for an entire audio file\n",
    "this function gets the librosa features for a clip using the extract_feature fxn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" this function gets the librosa features for a clip using the extract_feature fxn \"\"\"\n",
    "\n",
    "def get_librosa_feat(a, filename):\n",
    "        \n",
    "#     a.dropna(axis=0, how='any', inplace=True)\n",
    "    a.query('Duration_ms > 0.0', inplace=True)      \n",
    "    \n",
    "    song = AudioSegment.from_wav(filename)\n",
    "    merged_clips = song[0:0]\n",
    "    \n",
    "    for index, col in a.iterrows():\n",
    "        \n",
    "        start_clip = int(col['Start Time: ']*1000)\n",
    "        end_clip = int(math.ceil(col['End Time: ']*1000))\n",
    "        \n",
    "        # get duration in seconds\n",
    "        duration = (col[\"Duration_ms\"]/1000)\n",
    "        \n",
    "        if duration > 0:\n",
    "\n",
    "            # get start time in seconds\n",
    "            start_s = start_clip/1000 \n",
    "\n",
    "            # print (duration, start_s)\n",
    "            # print (start_clip, end_clip)\n",
    "\n",
    "            # read clip in file by start, end\n",
    "            read_clip = song[start_clip:end_clip]\n",
    "\n",
    "            try: \n",
    "                mfccs, chroma, mel, contrast, tonnetz = extract_feature(filename, start_s, duration)\n",
    "                \"\"\" get Librosa features \"\"\"\n",
    "\n",
    "                feature_list = [mfccs, chroma, mel, contrast, tonnetz]\n",
    "                feature_name_list = ['mfccs', 'chroma', 'mel', 'contrast', 'tonnetz']  \n",
    "                list_num = 0        \n",
    "                for feature in feature_list:\n",
    "                    # print (feature.shape)\n",
    "                    feature = feature.reshape(feature.shape[0], 1)\n",
    "\n",
    "                    try:\n",
    "                        x = feature\n",
    "\n",
    "                        # convert to dataframe type\n",
    "                        x_df = pd.DataFrame(x, columns=['0'])\n",
    "\n",
    "                        for idx, row in x_df.iterrows():\n",
    "                            # add new empty column to dataframe\n",
    "                            a.loc[index,str(str(feature_name_list[list_num])+'_'+str(idx+1))]= row['0']\n",
    "\n",
    "                            # add new value at inx in that column \n",
    "        #                     a.set_value[index, [str(str(feature_name_list[list_num])+'_'+str(idx+1))]]= row['0']\n",
    "        #                     new_row = a.loc[index, str(str(feature_name_list[list_num])+'_'+str(idx+1))]= row['0']\n",
    "        #                     new_row.loc[index, str(str(feature_name_list[list_num])+'_'+str(idx+1))] = row['0']\n",
    "                        list_num +=1\n",
    "                    except: \n",
    "                        a[counter, str(str(feature)+'_'+str(counter))] = 'NaN'\n",
    "                        list_num +=1\n",
    "                        pass\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "#         new_row = pd.DataFrame(columns=[\"Start Time: \", \"End Time: \", \"Call ID #: \", \"Call Name: \"])\n",
    "\n",
    "#         if 2*row+1 < len(audfile_start_end_id_df.index):\n",
    "#             new_row.loc[0] =[audfile_start_end_id_df.loc[row_count, 'End Time: '], \n",
    "#                      audfile_start_end_id_df.loc[row_count+1, 'Start Time: '], \n",
    "#                      0.0, 'no_call']\n",
    "#             audfile_start_end_id_df = pd.concat([audfile_start_end_id_df.loc[:row_count], new_row, audfile_start_end_id_df.loc[row_count+1:]])\n",
    "                \n",
    "#     a.dropna(axis=0, how='any', inplace=True)\n",
    "    a.query('Duration_ms > 0.0', inplace=True)        \n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T20:46:34.797594Z",
     "start_time": "2017-11-13T20:40:14.870Z"
    }
   },
   "source": [
    "#### iterate through all audio files (must be in single folder), saves data to a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "This function:\n",
    "\n",
    "    1. gets all audio files in a given folder (directory) \n",
    "    2. interate through each .wav file \n",
    "    3. appends data to a new dataframe\n",
    "    4. adds features \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def get_all_file_times(folder): \n",
    "    \n",
    "    # initialize dataframe\n",
    "    master_dataframe = pd.DataFrame(columns=[\"Start Time: \", \"End Time: \", \"Call ID #: \", \"Call Name: \"])\n",
    "    \n",
    "    print (master_dataframe)\n",
    "    \n",
    "    # iterate through all files, merge together\n",
    "\n",
    "    os.chdir(folder)\n",
    "    for filename in glob.glob(\"*wav\"):\n",
    "        aud_labels = pd.read_csv(r'\\audfile_labels.csv', sep=',',header=None)\n",
    "\n",
    "        if filename in pd.Series(aud_labels.loc[:,0]).values:\n",
    "            filenumber = aud_labels.index[aud_labels.loc[:,0] == filename].tolist()\n",
    "\n",
    "            print(filename)\n",
    "\n",
    "            # get start, end time, add blank sections\n",
    "#             filename_dataframe = get_file_start_end_id_add(filename)\n",
    "            \n",
    "            \"\"\" get features here BUT don't save to CSV! \"\"\"\n",
    "            # add welch freq & clip duration\n",
    "            filename_dataframe = add_freq_dur_to_df(filename)\n",
    "            \n",
    "            filename_dataframe.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "            print ('\\nfinished loading welch features\\n')\n",
    "            \n",
    "            # add librosa features\n",
    "            filename_dataframe = get_librosa_feat(filename_dataframe, filename)\n",
    "            \n",
    "            filename_dataframe.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "            print ('\\nfinished loading Librosa features\\n')\n",
    "            \n",
    "            \"\"\" save to master dataframe \"\"\"\n",
    "            # add info to master dataframe\n",
    "            master_dataframe = pd.concat([master_dataframe, filename_dataframe])\n",
    "            master_dataframe.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        else:\n",
    "            print (\"\\n\", filename, \" is not in list of approved files. Skipping.. \\n\")\n",
    "            \n",
    "        print (master_dataframe)\n",
    "        \n",
    "        master_dataframe.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    \"\"\" save to CSV here \"\"\"  \n",
    "    print (\"\\nsaving to CSV......\")\n",
    "    \n",
    "    master_dataframe.to_csv('new_master_datafrme_features.csv', sep=',')\n",
    "    \n",
    "    print ('Fin.')  \n",
    "    \n",
    "    return master_dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### audio playback functions\n",
    "these functions are used later to playback audio files in their entirety, by index, call, or number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T20:46:48.301948Z",
     "start_time": "2017-11-17T20:46:48.141522Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\" gets audio file info - duration, sample rate, total samples \"\"\"\n",
    "def get_aud_info(filename):\n",
    "    X, sr = librosa.load(filename, sr=None)\n",
    "    array_sounds = np.array(X)\n",
    "    duration = librosa.get_duration(y=X)\n",
    "    tot = array_sounds.shape[0]\n",
    "    print (\"Duration: \"+ str(duration)+\" seconds\")\n",
    "    print (\"Sampling Rate: \"+ str(sr) +\" Hz\")\n",
    "    print (\"Total # of Samples = \"  + str(tot) )\n",
    "    return duration, sr, tot\n",
    "\n",
    "\"\"\" plays back requested vocalization by call id or number \"\"\"\n",
    "def play_voc_by_call(filename, call_id_or_number ):\n",
    "    # get filename from previous entry\n",
    "    a = get_file_start_end_id(filename)\n",
    "    \n",
    "    #get chitter by call id or number \n",
    "    chitter_id, chitter_number = get_chitter_id_num(call_id_or_number)\n",
    "    \n",
    "    # get start time\n",
    "    df_start_list = a.loc[a['Call ID #: '] == chitter_number]['Start Time: ']\n",
    "    # print (df_start_list)\n",
    "\n",
    "    # get end times\n",
    "    df_end_list = a.loc[a['Call ID #: '] == chitter_number]['End Time: ']\n",
    "    # print (df_end_list)\n",
    "\n",
    "    # merge both times to single series\n",
    "    result = pd.concat([df_start_list, df_end_list], axis=1)\n",
    "    #print (result)\n",
    "\n",
    "    clips_list = []\n",
    "    song = AudioSegment.from_wav(filename)\n",
    "    merged_clips = song[0:0]\n",
    "    \n",
    "    import math \n",
    "    \n",
    "    for index, row in result.iterrows():\n",
    "        print (row['Start Time: '], row['End Time: '])    # add start, end time to a new list\n",
    "\n",
    "        start_clip = int(row['Start Time: ']*1000)\n",
    "        end_clip = int(math.ceil(row['End Time: ']*1000))\n",
    "        print (start_clip, end_clip)\n",
    "        \n",
    "        # next clip based on current iteration through the series\n",
    "        next_clip = song[start_clip:end_clip]\n",
    "\n",
    "        merged_clips += next_clip\n",
    "        \n",
    "        # temp_list = [row['Start Time: ']*1000, row['End Time: ']*1000]\n",
    "        # clips_list.extend(temp_list)\n",
    "        # print (clips_list)\n",
    "    \n",
    "    # playback, back-to-back, all indexed calls\n",
    "    \n",
    "#     song = AudioSegment.from_wav(merged_clips)\n",
    "#     slice = song[start_time, end_time]\n",
    "    play(merged_clips)\n",
    "    return merged_clips\n",
    "\n",
    "    \n",
    "\"\"\"  plays back requested vocalization by index number (order it occurs in file) \"\"\"\n",
    "\n",
    "def play_voc_by_index(filename, index):\n",
    "    # get filename from previous entry\n",
    "    a = get_file_start_end_id(filename)\n",
    "    \n",
    "    import math\n",
    "    # index a by call\n",
    "    start_time =a.loc[index, \"Start Time: \"]\n",
    "    end_time = a.loc[index, \"End Time: \"]\n",
    "    start_time = int(start_time*1000)\n",
    "    end_time = int(math.ceil(end_time*1000))\n",
    "    start_time = (start_time)\n",
    "    end_time = (end_time)\n",
    "    print (\"Start Time:  \" +str(start_time))\n",
    "    print (\"End Time:    \" +str(end_time))\n",
    "    \n",
    "    # load audio/video file\n",
    "    song = AudioSegment.from_wav(filename)\n",
    "    slice = song[start_time : end_time]\n",
    "    play(slice)\n",
    "    return slice\n",
    " \n",
    "\"\"\"plays back requested vocalization by filename & given start & end time \"\"\"\n",
    "\n",
    "def play_voc_by_start_end(filename, start_time, end_time):\n",
    "    start_time = (start_time)\n",
    "    end_time = (end_time)\n",
    "    song = AudioSegment.from_wav(filename)\n",
    "    slice = song[start_time : end_time]\n",
    "    play(slice)\n",
    "\n",
    "    \n",
    "\"\"\"plays back entire file  given \"\"\"\n",
    "\n",
    "def play_whole_file(filename):\n",
    "    song = AudioSegment.from_wav(filename)\n",
    "    slice = song[0:]\n",
    "    play(slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T18:32:42.932098Z",
     "start_time": "2017-11-13T18:32:42.929090Z"
    },
    "collapsed": true,
    "heading_collapsed": true
   },
   "source": [
    "# Audio Playback\n",
    "These cells perform the given action in their description with the given inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### get aud file info\n",
    " entire a valid filename bewteen the parentheses after \"filename = \"\n",
    " \n",
    " Note: if the file is not in the same directory as this .ipynb file, specify the full path + file\n",
    " for example - \"C:/Users/you/1203_1007_24hr.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\" example \"\"\"\n",
    "filename = \"1203_1007_24hr.wav\"\n",
    "aud_info = get_aud_info(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T20:47:52.471973Z",
     "start_time": "2017-11-17T20:47:52.420837Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:\\\\Users\\\\leviz\\\\OneDrive\\\\School\\\\PhD\\\\projects\\\\vocalization_classification\\\\pyAudioAnalysis_test'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-5b86000a7c4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0maud_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_aud_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-a3b587c7c4fe>\u001b[0m in \u001b[0;36mget_aud_info\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;34m\"\"\" gets audio file info - duration, sample rate, total samples \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_aud_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0marray_sounds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mduration\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_duration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\librosa\\core\\audio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0maudioread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maudio_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrealpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0minput_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m         \u001b[0msr_native\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamplerate\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[0mn_channels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchannels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\audioread\\__init__.py\u001b[0m in \u001b[0;36maudio_open\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrawread\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mrawread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRawAudioFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mDecodeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\audioread\\rawread.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \"\"\"\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'C:\\\\Users\\\\leviz\\\\OneDrive\\\\School\\\\PhD\\\\projects\\\\vocalization_classification\\\\pyAudioAnalysis_test'"
     ]
    }
   ],
   "source": [
    "\n",
    "filename = \"\"\n",
    "aud_info = get_aud_info(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Playback by Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T20:46:34.704346Z",
     "start_time": "2017-11-13T20:46:34.666735Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_file_start_end_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-54e42019a194>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mindex_playback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplay_voc_by_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-23-6488135d103b>\u001b[0m in \u001b[0;36mplay_voc_by_index\u001b[1;34m(filename, index)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplay_voc_by_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;31m# get filename from previous entry\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_file_start_end_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_file_start_end_id' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\" example \"\"\"\n",
    "\n",
    "# enter filename - including path if not in local directory\n",
    "filename = \"1203_1007_24hr.wav\"\n",
    "\n",
    "# playback by index - run 'get_file_start_end_id(filename)' to get indices\n",
    "index = 3\n",
    "\n",
    "index_playback = play_voc_by_index(filename, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T20:46:34.704346Z",
     "start_time": "2017-11-13T20:46:34.666735Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_file_start_end_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-54e42019a194>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mindex_playback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplay_voc_by_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-23-6488135d103b>\u001b[0m in \u001b[0;36mplay_voc_by_index\u001b[1;34m(filename, index)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplay_voc_by_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;31m# get filename from previous entry\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_file_start_end_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_file_start_end_id' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\" playback by index number \"\"\"\n",
    "\n",
    "# enter filename - including path if not in local directory\n",
    "filename = \"\"\n",
    "\n",
    "# playback by index - run 'get_file_start_end_id(filename)' to get indices\n",
    "index = 3\n",
    "\n",
    "index_playback = play_voc_by_index(filename, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Playback entire audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T20:46:34.704346Z",
     "start_time": "2017-11-13T20:25:51.965Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\" example \"\"\"\n",
    "\n",
    "# enter filename - including path if not in local directory\n",
    "filename = \"1203_1007_24hr.wav\"\n",
    "\n",
    "whole_file = play_whole_files(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T20:46:34.704346Z",
     "start_time": "2017-11-13T20:25:51.965Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# enter filename - including path if not in local directory\n",
    "filename = \"\"\n",
    "\n",
    "whole_file = play_whole_files(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Playback by Call\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\" example \"\"\"\n",
    "\n",
    "# enter filename - including path if not in local directory\n",
    "filename = \"1203_1007_24hr.wav\"\n",
    "\n",
    "# enter a valid call type - use 'print (id_dict)' to get calls\n",
    "call_id_or_number = 21.0\n",
    "\n",
    "playback_by_call = play_voc_by_call(filename, call_id_or_number) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\" playback by call \"\"\"\n",
    "\n",
    "# enter filename - including path if not in local directory\n",
    "filename = \"\"\n",
    "\n",
    "# enter a valid call type - use 'print (id_dict)' to get calls\n",
    "call_id_or_number = \n",
    "\n",
    "playback_by_call = play_voc_by_call(filename, call_id_or_number) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "source": [
    "### Playback by Start & End Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\" example \"\"\"\n",
    "\n",
    "# enter filename - including path if not in local directory\n",
    "filename = \"1203_1007_24hr.wav\"\n",
    "\n",
    "# playback by start & end times (ms) - run 'get_file_start_end_id(filename)' to get times\n",
    "start_time = 20213\n",
    "end_time = 20820\n",
    "\n",
    "playback_by_start_end = play_voc_by_start_end(filename, start_time, end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\" playback by specific start & end times \"\"\"\n",
    "\n",
    "# enter filename - including path if not in local directory\n",
    "filename = \"\"\n",
    "\n",
    "# playback by start & end times (ms) - run 'get_file_start_end_id(filename)' to get times\n",
    "start_time =\n",
    "end_time =\n",
    "\n",
    "playback_by_start_end = play_voc_by_start_end(filename, start_time, end_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Features\n",
    "Extract Features from audiofiles & save to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save features to a CSV for training classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" example \"\"\"\n",
    "\n",
    "\"\"\" take all audio files from a given folder & save to CSV \"\"\"\n",
    "\n",
    "folder = (r'C:/Users/leviz/Desktop/audio_files/')\n",
    "\n",
    "get_master_data = get_all_file_times(folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" take all audio files from a given folder & save to CSV \"\"\"\n",
    "\n",
    "# enter folder path b/w \"\" after r\n",
    "folder = (r\"\")\n",
    "\n",
    "get_master_data = get_all_file_times(folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Vocalization Classifier\n",
    "Run & train classifiers to predict vocalizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# load CSV with all features for all aud files in specified folder saved previously \n",
    "call_dataframe = pd.read_csv(r'/full_master_dataframe_features.csv')\n",
    "\n",
    "# show the first few rows\n",
    "display(call_dataframe.head(n=5))\n",
    "\n",
    "# function to randomly split data into training and test sets\n",
    "def shuffle_split_data(X, y, test_size, random_state ):\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=test_size, random_state=random_state) \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "# drop sounds that show up with low freq - approx. < once per file on avg\n",
    "drop_list = ['pain', 'high freq squeal', 'squeak','high freq warble', 'wheep', 'pip',\n",
    "             'high freq chirp', 'low whine', 'loud whine', 'high freq loud whistle', \n",
    "             'grunt', 'low freq whistle', 'long whine', 'whine/warble', 'no call']\n",
    "\n",
    "# drop all rows in dataset with voc. in drop list\n",
    "data_set = call_dataframe\n",
    "for call in drop_list:\n",
    "    mask = data_set['Call Name: '] == call\n",
    "    data_set = data_set[~mask]\n",
    "    \n",
    "# set predictor data columns, drop labels\n",
    "X_all = data_set.drop('Call Name: ', axis=1)\n",
    "X_all.drop('Call ID #: ', axis=1, inplace=True)\n",
    "X_all.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "# standardize  data\n",
    "normalized_X = preprocessing.Normalizer().fit(X_all)\n",
    "\n",
    "call_dataframe = X_all\n",
    "\n",
    "# set label data for testing \n",
    "y_all = data_set.loc[:, 'Call Name: ']\n",
    "df = pd.DataFrame({'call_name':list(y_all)})\n",
    "\n",
    "call_label_dataframe = y_all\n",
    "\n",
    "print (df)\n",
    "\n",
    "# print total number of voc. labels in dataset\n",
    "df['call_name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Initialize all functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T20:43:32.623018Z",
     "start_time": "2017-11-20T20:43:32.620011Z"
    },
    "hidden": true
   },
   "source": [
    "### Initialize all classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Initialize the models\n",
    "clf_A = svm.SVC(random_state=0)\n",
    "\n",
    "clf_B = LogisticRegression(fit_intercept=False, n_jobs=1)\n",
    "clf_C = PassiveAggressiveClassifier(max_iter=10, tol=None)\n",
    "\n",
    "clf_D = KNeighborsClassifier(n_neighbors=2)\n",
    "\n",
    "clf_E = AdaBoostClassifier()\n",
    "clf_F = RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)\n",
    "clf_G = ExtraTreesClassifier()\n",
    "clf_H = GradientBoostingClassifier()\n",
    "\n",
    "clf_I = GaussianNB()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Initialize all classifier functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def train_classifier(clf, X_train, y_train):\n",
    "    ''' Fits a classifier to the training data. '''\n",
    "    \n",
    "    # Start the clock, train the classifier, then stop t\"he clock\n",
    "    start = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time()\n",
    "    \n",
    "    # Print the results\n",
    "    print (\"Trained model in {:.4f} seconds\".format(end - start))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    \n",
    "def predict_labels(clf, features, target):\n",
    "    ''' Makes predictions using a fit classifier based on F1 score. '''\n",
    "    \n",
    "    # Start the clock, make predictions, then stop the clock\n",
    "    start = time()\n",
    "    y_pred = clf.predict(features)\n",
    "    end = time()\n",
    "    \n",
    "    # Print and return results\n",
    "    print (\"Made predictions in {:.4f} seconds.\".format(end - start))\n",
    "    return f1_score(target, y_pred, average='weighted')\n",
    "\n",
    "def accuracy_predict_labels(clf, features, target):\n",
    "    ''' Makes predictions using a fit classifier based on F1 score. '''\n",
    "    \n",
    "    # Start the clock, make predictions, then stop the clock\n",
    "    start = time()\n",
    "    y_pred = clf.predict(features)\n",
    "    end = time()\n",
    "    \n",
    "    # Print and return results\n",
    "    print (\"Made predictions in {:.4f} seconds.\".format(end - start))\n",
    "    return accuracy_score(target, y_pred)\n",
    "\n",
    "def precision_predict_labels(clf, features, target):\n",
    "    ''' Makes predictions using a fit classifier based on F1 score. '''\n",
    "    \n",
    "    # Start the clock, make predictions, then stop the clock\n",
    "    start = time()\n",
    "    y_pred = clf.predict(features)\n",
    "    end = time()\n",
    "    \n",
    "    # Print and return results\n",
    "    print (\"Made predictions in {:.4f} seconds.\".format(end - start))\n",
    "    return precision_score(target, y_pred, average='weighted')\n",
    "\n",
    "\n",
    "def train_predict(clf, X_train, y_train, X_test, y_test):\n",
    "    ''' Train and predict using a classifer based on F1 score. '''\n",
    "    \n",
    "    # Indicate the classifier and the training set size\n",
    "    print (\"Training a {} using a training set size of {}. . .\".format(clf.__class__.__name__, len(X_train)))\n",
    "    \n",
    "    # Train the classifier\n",
    "    train_classifier(clf, X_train, y_train)\n",
    "    \n",
    "    # Print the results of prediction for both training and testing\n",
    "    print (\"F1 score for training set: {:.4f}.\".format(predict_labels(clf, X_train, y_train)))\n",
    "    print (\"F1 score for test set: {:.4f}.\".format(predict_labels(clf, X_test, y_test)))\n",
    "    print ('\\n')\n",
    "    print (\"Accuracy score for training set: {:.4f}.\".format(accuracy_predict_labels(clf, X_train, y_train)))\n",
    "    print (\"Accuracy score for test set: {:.4f}.\".format(accuracy_predict_labels(clf, X_test, y_test)))\n",
    "    print ('\\n')\n",
    "    print (\"Precision score for training set: {:.4f}.\".format(precision_predict_labels(clf, X_train, y_train)))\n",
    "    print (\"Precision score for test set: {:.4f}.\".format(precision_predict_labels(clf, X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T20:47:17.407166Z",
     "start_time": "2017-11-20T20:47:17.349011Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\" a fxn to run a grid_search on a single classifer \"\"\"\n",
    "\n",
    "def run_single(X_train, X_test, y_train, y_test, clf_key, clf_value, scoring):\n",
    "    t0 = time()\n",
    "\n",
    "    grid_param = clf_param_dict[clf_key]\n",
    "\n",
    "    gridcv = model_selection.GridSearchCV(estimator=clf_value, param_grid=grid_param, n_jobs=100, scoring=scoring)\n",
    "\n",
    "    gridcv.fit(X_train, y_train)\n",
    "    y_predictions = gridcv.predict(X_test)\n",
    "\n",
    "    best_est = gridcv.best_estimator_\n",
    "    print(\"\\ndone in %0.3fs\" % (time() - t0))\n",
    "    print(\"Best estimator found by grid search:\")\n",
    "    print(best_est)      \n",
    "\n",
    "    \"\"\" report score for each classifier \"\"\"\n",
    "    report = sklearn.metrics.classification_report( y_test, y_predictions )\n",
    "    print (\"\\n\", report, \"\\n\")\n",
    "\n",
    "    \"\"\" create confusion matrix for best classifier \"\"\"\n",
    "    \n",
    "    display(Image('https://i.stack.imgur.com/AuTKP.png'))\n",
    "    class_names = y_all.unique().tolist()\n",
    "    viz = ConfusionMatrix(best_est, classes=class_names)\n",
    "    viz.fit(X_train, y_train)\n",
    "    viz.score(X_test, y_test)\n",
    "    viz.poof()\n",
    "    \n",
    "    \n",
    "    \"\"\" show feature importance \"\"\"\n",
    "    result = best_est\n",
    "    try: \n",
    "        # Get Feature Importance from the classifier\n",
    "        best_est = result.fit(X_train, y_train)\n",
    "        feature_importance = best_est.feature_importances_\n",
    "        # Normalize The Features\n",
    "        feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "        sorted_idx = np.argsort(feature_importance)\n",
    "        pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "        plt.figure(figsize=(16, 90))\n",
    "        plt.barh(pos, feature_importance[sorted_idx], align='center', color='#7A68A6')\n",
    "        plt.yticks(pos, np.asanyarray(call_dataframe.columns.tolist())[sorted_idx])\n",
    "        plt.xlabel('Relative Importance')\n",
    "        plt.title('Variable Importance')\n",
    "        plt.show()\n",
    "    except:\n",
    "        print ('\\nCannot show feature importance w/ this Classifier')\n",
    "    return best_est\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T20:42:05.520358Z",
     "start_time": "2017-11-20T20:42:05.517350Z"
    },
    "hidden": true
   },
   "source": [
    "### create dictionary for grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create classifier dictionary\n",
    "clf_dict = {\n",
    "            'SVM':clf_A,\n",
    "            'LogisticRegression':clf_B,\n",
    "            'PassiveAggressiveClassifier':clf_C,\n",
    "            'KNN':clf_D,\n",
    "            'AdaBoost':clf_E,\n",
    "            'RandomForest':clf_F,\n",
    "            'ExtraTrees':clf_G,\n",
    "            'GradientBoosting':clf_H,\n",
    "            'GaussianNB':clf_I\n",
    "            }\n",
    "\n",
    "grid_params = dict(feature_selection__k=[100, 200], \n",
    "              random_forest__n_estimators=[50, 100, 200],\n",
    "              random_forest__min_samples_split=[2, 3, 4, 5, 10])\n",
    "\n",
    "clf_param_dict = {\n",
    "            'SVM':[  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "              {'C': [1, 10, 100, 1000], 'gamma': [0.1, 0.01, 0.001, 0.0001], 'kernel': ['rbf']}],\n",
    "            'LogisticRegression':{'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]},\n",
    "            'PassiveAggressiveClassifier': {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]},\n",
    "            'KNN':{'n_neighbors': np.arange(20)+1},\n",
    "            'AdaBoost':{ 'n_estimators': [16, 32] },\n",
    "            'RandomForest':{ 'n_estimators': [16, 32],  'max_features':[0,1,2,5,10, 50, 100] },\n",
    "            'ExtraTrees':{ 'n_estimators': [16, 32] },\n",
    "            'GradientBoosting':{ 'n_estimators': [16, 32, 64, 128],\n",
    "                                'max_depth': [3, 5, 10], 'learning_rate': [0.01, 0.1, 0.2, 0.8, 1.0]},\n",
    "            'GaussianNB':{}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Test Classifiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\" test classifiers WITHOUT running a grid_search \"\"\"\n",
    "\n",
    "# loop thru models, then thru train sizes\n",
    "for clf in [clf_A, clf_B, clf_C, clf_D, clf_E, clf_F, clf_G, clf_H, clf_I]:\n",
    "    print (\"\\n{}: \\n\".format(clf.__class__.__name__))\n",
    "    y = train_predict(clf, X_train, y_train, X_test, y_test)\n",
    "    print (\"------------------------------------------\")\n",
    "    print (\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### test a single classifiers WITH a grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\" example \"\"\"\n",
    "\n",
    "# specify train / test data sizes and initial state\n",
    "test_size = 0.20\n",
    "random_state = 20\n",
    "\n",
    "# set scoring type  - http://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "scoring = 'f1_weighted'\n",
    "\n",
    "#  select a classifier from the following list\n",
    "# 'SVM', 'LogisticRegression', 'PassiveAggressiveClassifier', 'KNN','AdaBoost', \n",
    "# 'RandomForest', extraTrees', 'GradientBoosting',or 'GaussianNB'\n",
    "\n",
    "clf_key = 'SVM'\n",
    "clf_value = clf_dict[clf_key]\n",
    "\n",
    "X_train, X_test, y_train, y_test = shuffle_split_data(X_all, y_all, test_size, random_state)\n",
    "result = run_single(X_train, X_test, y_train, y_test, clf_key, clf_value, scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# specify train / test data sizes (enter value 0 to 1) and initial state\n",
    "test_size = \n",
    "random_state = \n",
    "\n",
    "# set scoring type  - http://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "scoring = ''\n",
    "\n",
    "#  select a classifier from the following list\n",
    "# 'SVM', 'LogisticRegression', 'PassiveAggressiveClassifier', 'KNN','AdaBoost', \n",
    "# 'RandomForest', extraTrees', 'GradientBoosting',or 'GaussianNB'\n",
    "\n",
    "clf_key = ''\n",
    "clf_value = clf_dict[clf_key]\n",
    "\n",
    "X_train, X_test, y_train, y_test = shuffle_split_data(X_all, y_all, test_size, random_state)\n",
    "result = run_single(X_train, X_test, y_train, y_test, clf_key, clf_value, scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Detect Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Get Feature Importance from the classifier\n",
    "best_est = result.fit(X_train, y_train)\n",
    "feature_importance = best_est.feature_importances_\n",
    "\n",
    "# Normalize The Features\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "\n",
    "# plot the results\n",
    "plt.figure(figsize=(12, 120))\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center', color='#7A68A6')\n",
    "plt.yticks(pos, np.asanyarray(call_dataframe.columns.tolist())[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {
    "height": "906px",
    "left": "0px",
    "right": "1677px",
    "top": "106px",
    "width": "161px"
   },
   "toc_section_display": "none",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
